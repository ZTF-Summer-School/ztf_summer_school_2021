{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 1: Building a real-bogus classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, you will apply your (newly acquired) knowledge of deep learning 101\n",
    "to build a classifier for a real life problem (well, actually not, but almost\n",
    "-- see the very end of this exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_base = './'\n",
    "path_data = './data'\n",
    "path_model = './models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The problem: the `rbp` classifier for the `GTF`\n",
    "\n",
    " You are running a state-of-the-art *robotic* sky survey called the Geeky Transient Facility,\n",
    " or GTF, perhaps somewhere in the Southern Hemisphere.\n",
    " You have a fancy image-differencing pipeline that allows you to find interesting objects in the\n",
    " dynamic sky every night, such as supernovae, variable stars, and asteroids, and send them\n",
    " out to the world as alerts.\n",
    "\n",
    " However you notice two things:\n",
    "\n",
    " 1. Your fancy image-differencing pipeline produces a lot of bogus detections.\n",
    " With the increasing data rates you are worried that one day you will run out of\n",
    " graduate students to examine/scan the potentially interesting candidate sources.\n",
    "\n",
    " 2. Your AGI-based robot-telescope is extraordinarily sophisticated and smart,\n",
    " but has one major soft spot: it really likes platypuses. In fact, the robot likes them\n",
    " so much (obsessed, really) that whenever a platypus appears in the vicinity, it halts observing\n",
    " and instead takes a picture of it and sends it out as an alert.\n",
    "\n",
    "So you decide to build a `real, bogus, or platypus` (`rbp` for short) classifier for GTF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "You are not the first one to encounter a problem like this, so you decide to apply the\n",
    "[*transfer learning*](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "technique and use the real-bogus classifier `braai` that is employed\n",
    "by a similar survey called [ZTF](https://ztf.caltech.edu) running in the Northern Hemisphere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the necessities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from astropy.visualization import ZScaleInterval\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The model\n",
    "\n",
    "Download a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_model = 'models/'\n",
    "model_name = 'braai_d6_m9.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 18:05:17.405875: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(os.path.join(path_model,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VGG6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 61, 61, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 59, 59, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 29, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 29, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 27, 27, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 25, 25, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "fc_1 (Dense)                 (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc_out (Dense)               (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 312,081\n",
      "Trainable params: 312,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be using the [functional API of `tf.keras`](https://www.tensorflow.org/guide/keras/functional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Remove the output layer, leave the feature extraction part of the network in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model_fe = tf.keras.Model(inputs=model.inputs, outputs=...\n",
    "model_fe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Create a new output layer that will provide the probabilities for our three classes.\n",
    "\n",
    "Use tf.keras.layers.Dense with 3 ``units``, representing the dimensionality of the output space.ew output layer that will provide the probabilities for our three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "output = tf.keras.layers.Dense(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_tl = tf.keras.Model(inputs=model_fe.inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: You can now compile the model and it is ready for training!\n",
    "\n",
    "The new model architecture is ready! You may optionally \"freeze\" the weights in the feature-extracting\n",
    "part of the network and only train the \"head\" of the network that does the classification -- feel\n",
    "free to play around with this once you're done with the exercise.\n",
    "\n",
    "Compile the model with the ``adam`` optimizer, ``categorical_crossentropy`` loss, and ``BinaryAccuracy, Precision, Recall, and AUC`` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# mark layers as not trainable\n",
    "# for layer in model_tl.layers[:-1]:\n",
    "# \tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model_tl.compile(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "You have collected training examples and now need to make a dataset out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = pathlib.Path('data')\n",
    "\n",
    "bogus = np.array([np.load(pp) for pp in (p / 'bogus').glob('*.npy')])\n",
    "real = np.array([np.load(pp) for pp in (p / 'real').glob('*.npy')])\n",
    "platypus = np.array([np.load(pp) for pp in (p / 'platypus').glob('*.npy')])\n",
    "\n",
    "data = np.vstack((bogus, real, platypus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 63, 63, 3), (100, 63, 63, 3), (51, 63, 63, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bogus.shape, real.shape, platypus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Make labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indices = [0 for _ in range(bogus.shape[0])] +\\\n",
    "    [1 for _ in range(real.shape[0])] +\\\n",
    "    [2 for _ in range(platypus.shape[0])]\n",
    "labels = tf.one_hot(indices, depth=3, dtype=tf.int8).numpy()\n",
    "# labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 4: Perform the train/validation/test split (we will use an 81\\% / 9\\% / 10\\% data split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "val_size = 0.1\n",
    "random_state = 42\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_indexes, test_indexes = train_test_split(...\n",
    "train_indexes, val_indexes = train_test_split(train_indexes, shuffle=True,\n",
    "                                              test_size=val_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "shuffle_buffer_size = 4\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will feed the model with data stored as `tf.data.Dataset`'s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((data[train_indexes], labels[train_indexes]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((data[val_indexes], labels[val_indexes]))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((data[test_indexes], labels[test_indexes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now shuffle, batch, and repeat the data. Let's also apply some data augmentation (random horizontal\n",
    "and vertical flips) to make the classifier more robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size).repeat(epochs)\n",
    "# flipping:\n",
    "train_dataset = train_dataset.cache().map(\n",
    "    lambda image, label: (tf.image.random_flip_left_right(image), label)\n",
    ").map(\n",
    "    lambda image, label: (tf.image.random_flip_up_down(image), label)\n",
    ").shuffle(shuffle_buffer_size).batch(batch_size).repeat(epochs)\n",
    "val_dataset = val_dataset.batch(batch_size).repeat(epochs)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to compute for how many epochs to train, validate, and test the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 4, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train = len(train_indexes) // batch_size - 1\n",
    "steps_per_epoch_val = len(val_indexes) // batch_size - 1\n",
    "steps_per_epoch_test = len(test_indexes) // batch_size - 1\n",
    "steps_per_epoch_train, steps_per_epoch_val, steps_per_epoch_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: We don't want to [overfit](https://en.wikipedia.org/wiki/Overfitting), so we will halt training if the loss on the validation dataset is not improving for, say 5 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6543b72c54f14e4ca1c41e662fad3a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(...\n",
    "\n",
    "tqdm_callback = TqdmCallback(verbose=1)\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping_callback,\n",
    "    tqdm_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 6: We are ready to fit and evaluate the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 18:06:23.603365: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "h = model_tl.fit(\n",
    "    train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch_train,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_dataset, validation_steps=steps_per_epoch_val,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The model is trained and we can now evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step - loss: 2.6800e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6799776605912484e-05, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tl.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ll = {0: 'bogus', 1: 'real', 2: 'platypus'}\n",
    "\n",
    "for ii in test_indexes:\n",
    "    interval = ZScaleInterval()\n",
    "    limits = interval.get_limits(data[ii, ..., 0])\n",
    "\n",
    "    fig = plt.figure(figsize=(3, 3), dpi=100)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(data[ii, ..., 0], vmin=limits[0], vmax=limits[1])\n",
    "\n",
    "    p = np.argmax(model_tl.predict(np.expand_dims(data[ii], axis=0))[0])\n",
    "    display(f\"{ll[p]}:\")\n",
    "\n",
    "    plt.show()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Unsolicited wisdom\n",
    "\n",
    "Even though this may seem like some ridiculous nonsense, you've actually implemented a real-life\n",
    "transfer learning pipeline:\n",
    "you took an off-the-shelf production DL model, then using a part of it as a pre-trained feature extractor,\n",
    "adapted its architecture to a different problem, and trained on data the original model never saw, achieving\n",
    "very good results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 2: Deep learning for SCoPe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "We first load the data from the dataset file and save it in ```df_raw_data```. Then, we extract the features/labels used in the paper and save them in ```df_feats``` and ```df_labels``` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../scope/data/dataset.csv'\n",
    "df_raw_data = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_COL = [# 10 Cols for the phenomenological classifiers \n",
    "              'variable', 'periodic', 'long timescale', 'irregular', 'eclipsing', 'EA', 'EB', 'EW', 'flaring', 'bogus',\n",
    "              # new\n",
    "              'non-variable', 'dipping', 'blend', 'bright star', 'ccd artifact', 'galaxy', 'sinusoidal', 'sawtooth', \n",
    "              'elliptical',\n",
    "    \n",
    "    \n",
    "              # 13 Cols for the ontological classifiers\n",
    "              'pulsator', 'Delta Scu', 'Cepheid', 'RR Lyrae', 'LPV', 'Mira', 'SRV', 'binary star', 'W Uma', 'Beta Lyr',\n",
    "              'RS CVn', 'AGN', 'YSO',\n",
    "              # new\n",
    "              'F', 'O', 'Cepheid type-II', 'detached eclipsing MS-MS', 'compact binary', 'eclipsing WD+dM (NN Ser)',\n",
    "              'eclipsing sdB+dM (HW Vir)', 'RR Lyrae Blazhko', 'RR Lyrae ab', 'RR Lyrae c', 'RR Lyrae d', 'BL Her', \n",
    "              'RV Tau', 'W Virginis',  \n",
    "    \n",
    "              # unclear:\n",
    "              'double period', 'half period', 'multi periodic', 'nice', 'niice', 'wrong period',\n",
    "             ]\n",
    "\n",
    "FEATS_COL = [# 38 Cols for the phenomenological classifiers \n",
    "             'period', 'significance', 'n', 'median', 'wmean', 'wstd', 'chi2red', 'roms', 'norm_peak_to_peak_amp',\n",
    "             'norm_excess_var', 'median_abs_dev', 'iqr', 'f60', 'f70', 'f80' , 'f90', 'skew', 'smallkurt', \n",
    "             'inv_vonneumannratio', 'welch_i', 'stetson_j', 'stetson_k', 'ad', 'sw', 'f1_power', 'f1_bic','f1_amp', \n",
    "             'f1_phi0', 'f1_relamp1', 'f1_relphi1', 'f1_relamp2', 'f1_relphi2', 'f1_relamp3', 'f1_relphi3', 'f1_relamp4', \n",
    "             'f1_relphi5', 'n_ztf_alerts', 'mean_ztf_alert_braai',  \n",
    "             \n",
    "             # 30 Cols for the ontological classifiers\n",
    "             'AllWISE__w1mpro', 'AllWISE__w1sigmpro', 'AllWISE__w2mpro', 'AllWISE__w2sigmpro', 'AllWISE__w3mpro',\n",
    "             'AllWISE__w3sigmpro', 'AllWISE__w4mpro','AllWISE__w4sigmpro', 'AllWISE__ph_qual',\n",
    "             'Gaia_DR2__phot_g_mean_mag', 'Gaia_DR2__phot_bp_mean_mag', 'Gaia_DR2__phot_rp_mean_mag', 'Gaia_DR2__parallax',\n",
    "             'Gaia_DR2__parallax_error', 'Gaia_DR2__pmra', 'Gaia_DR2__pmra_error', 'Gaia_DR2__pmdec', 'Gaia_DR2__pmdec_error',\n",
    "             'Gaia_DR2__astrometric_excess_noise', 'Gaia_DR2__phot_bp_rp_excess_factor',\n",
    "             'PS1_DR1__gMeanPSFMag', 'PS1_DR1__gMeanPSFMagErr', 'PS1_DR1__rMeanPSFMag', 'PS1_DR1__rMeanPSFMagErr', \n",
    "             'PS1_DR1__iMeanPSFMag', 'PS1_DR1__iMeanPSFMagErr', 'PS1_DR1__zMeanPSFMag', 'PS1_DR1__zMeanPSFMagErr', \n",
    "             'PS1_DR1__yMeanPSFMag', 'PS1_DR1__yMeanPSFMagErr', 'PS1_DR1__qualityFlag']\n",
    "\n",
    "\n",
    "df_feats = df_raw_data[FEATS_COL]\n",
    "df_labels = df_raw_data[LABELS_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/hzdk6bx960n0xc_cxxz5n31r0000gq/T/ipykernel_72409/3673800119.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_feats['AllWISE__ph_qual'] = LabelEncoder().fit_transform(df_feats['AllWISE__ph_qual'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>significance</th>\n",
       "      <th>n</th>\n",
       "      <th>median</th>\n",
       "      <th>wmean</th>\n",
       "      <th>wstd</th>\n",
       "      <th>chi2red</th>\n",
       "      <th>roms</th>\n",
       "      <th>norm_peak_to_peak_amp</th>\n",
       "      <th>norm_excess_var</th>\n",
       "      <th>...</th>\n",
       "      <th>PS1_DR1__gMeanPSFMagErr</th>\n",
       "      <th>PS1_DR1__rMeanPSFMag</th>\n",
       "      <th>PS1_DR1__rMeanPSFMagErr</th>\n",
       "      <th>PS1_DR1__iMeanPSFMag</th>\n",
       "      <th>PS1_DR1__iMeanPSFMagErr</th>\n",
       "      <th>PS1_DR1__zMeanPSFMag</th>\n",
       "      <th>PS1_DR1__zMeanPSFMagErr</th>\n",
       "      <th>PS1_DR1__yMeanPSFMag</th>\n",
       "      <th>PS1_DR1__yMeanPSFMagErr</th>\n",
       "      <th>PS1_DR1__qualityFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.795541</td>\n",
       "      <td>-0.411116</td>\n",
       "      <td>1.056842</td>\n",
       "      <td>-0.080996</td>\n",
       "      <td>-0.070772</td>\n",
       "      <td>-0.729371</td>\n",
       "      <td>-0.204919</td>\n",
       "      <td>-0.618402</td>\n",
       "      <td>-0.556206</td>\n",
       "      <td>-0.090013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505132</td>\n",
       "      <td>-0.171156</td>\n",
       "      <td>-0.711477</td>\n",
       "      <td>-0.108664</td>\n",
       "      <td>-0.679382</td>\n",
       "      <td>-0.035408</td>\n",
       "      <td>-0.589965</td>\n",
       "      <td>0.087552</td>\n",
       "      <td>-0.428986</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.169144</td>\n",
       "      <td>-0.404608</td>\n",
       "      <td>-0.609176</td>\n",
       "      <td>1.717669</td>\n",
       "      <td>1.745705</td>\n",
       "      <td>0.605940</td>\n",
       "      <td>-0.203427</td>\n",
       "      <td>-0.567115</td>\n",
       "      <td>-0.058580</td>\n",
       "      <td>-0.067296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.771984</td>\n",
       "      <td>1.529579</td>\n",
       "      <td>2.328723</td>\n",
       "      <td>1.432994</td>\n",
       "      <td>0.796992</td>\n",
       "      <td>1.331472</td>\n",
       "      <td>0.413797</td>\n",
       "      <td>1.111349</td>\n",
       "      <td>-0.121882</td>\n",
       "      <td>-0.577069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.168778</td>\n",
       "      <td>-0.339781</td>\n",
       "      <td>-0.709640</td>\n",
       "      <td>1.355583</td>\n",
       "      <td>1.383824</td>\n",
       "      <td>0.625651</td>\n",
       "      <td>-0.194344</td>\n",
       "      <td>-0.467194</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>-0.022121</td>\n",
       "      <td>...</td>\n",
       "      <td>6.291251</td>\n",
       "      <td>1.346526</td>\n",
       "      <td>0.143841</td>\n",
       "      <td>1.067136</td>\n",
       "      <td>0.790237</td>\n",
       "      <td>0.829973</td>\n",
       "      <td>0.215383</td>\n",
       "      <td>0.617988</td>\n",
       "      <td>-0.170997</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.167949</td>\n",
       "      <td>-0.153682</td>\n",
       "      <td>-0.575688</td>\n",
       "      <td>0.376213</td>\n",
       "      <td>0.371072</td>\n",
       "      <td>0.035347</td>\n",
       "      <td>-0.164693</td>\n",
       "      <td>-0.161056</td>\n",
       "      <td>-0.089659</td>\n",
       "      <td>-0.064653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079213</td>\n",
       "      <td>0.424836</td>\n",
       "      <td>0.335221</td>\n",
       "      <td>0.589397</td>\n",
       "      <td>-0.315000</td>\n",
       "      <td>0.697595</td>\n",
       "      <td>-0.091769</td>\n",
       "      <td>0.706519</td>\n",
       "      <td>0.258703</td>\n",
       "      <td>-0.577069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.167962</td>\n",
       "      <td>-0.326416</td>\n",
       "      <td>-0.475225</td>\n",
       "      <td>-0.960383</td>\n",
       "      <td>-0.960730</td>\n",
       "      <td>-0.642459</td>\n",
       "      <td>-0.187080</td>\n",
       "      <td>-0.368344</td>\n",
       "      <td>-0.677252</td>\n",
       "      <td>-0.087730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414266</td>\n",
       "      <td>-0.674166</td>\n",
       "      <td>-0.685891</td>\n",
       "      <td>-0.570005</td>\n",
       "      <td>-0.178704</td>\n",
       "      <td>-0.493979</td>\n",
       "      <td>-0.272985</td>\n",
       "      <td>-0.272220</td>\n",
       "      <td>-0.641119</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>-0.168765</td>\n",
       "      <td>0.561914</td>\n",
       "      <td>0.043836</td>\n",
       "      <td>0.225089</td>\n",
       "      <td>0.251542</td>\n",
       "      <td>0.162925</td>\n",
       "      <td>-0.099879</td>\n",
       "      <td>0.132890</td>\n",
       "      <td>0.162960</td>\n",
       "      <td>-0.048426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408199</td>\n",
       "      <td>0.312270</td>\n",
       "      <td>-0.127323</td>\n",
       "      <td>0.312456</td>\n",
       "      <td>-0.503739</td>\n",
       "      <td>0.356175</td>\n",
       "      <td>0.212744</td>\n",
       "      <td>0.361095</td>\n",
       "      <td>-0.558559</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>-0.168783</td>\n",
       "      <td>0.347649</td>\n",
       "      <td>-0.575688</td>\n",
       "      <td>-0.698025</td>\n",
       "      <td>-0.672249</td>\n",
       "      <td>0.112984</td>\n",
       "      <td>0.255438</td>\n",
       "      <td>1.149209</td>\n",
       "      <td>-0.028490</td>\n",
       "      <td>-0.048638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943568</td>\n",
       "      <td>-0.446250</td>\n",
       "      <td>0.416117</td>\n",
       "      <td>-0.382705</td>\n",
       "      <td>0.143071</td>\n",
       "      <td>-0.370791</td>\n",
       "      <td>-0.387639</td>\n",
       "      <td>-0.145787</td>\n",
       "      <td>0.548454</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>-0.169044</td>\n",
       "      <td>-0.325871</td>\n",
       "      <td>-0.684524</td>\n",
       "      <td>0.065781</td>\n",
       "      <td>0.067812</td>\n",
       "      <td>-0.388981</td>\n",
       "      <td>-0.175824</td>\n",
       "      <td>-0.255117</td>\n",
       "      <td>-0.473654</td>\n",
       "      <td>-0.082420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115449</td>\n",
       "      <td>0.177809</td>\n",
       "      <td>0.047016</td>\n",
       "      <td>0.365860</td>\n",
       "      <td>-0.339711</td>\n",
       "      <td>0.478386</td>\n",
       "      <td>-0.262794</td>\n",
       "      <td>0.483536</td>\n",
       "      <td>-0.033011</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>-0.169061</td>\n",
       "      <td>-0.310516</td>\n",
       "      <td>-0.684524</td>\n",
       "      <td>0.418150</td>\n",
       "      <td>0.422739</td>\n",
       "      <td>-0.051247</td>\n",
       "      <td>-0.177880</td>\n",
       "      <td>-0.267395</td>\n",
       "      <td>-0.220118</td>\n",
       "      <td>-0.070384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203350</td>\n",
       "      <td>0.476170</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>0.662414</td>\n",
       "      <td>0.414358</td>\n",
       "      <td>0.802405</td>\n",
       "      <td>-0.355381</td>\n",
       "      <td>0.747981</td>\n",
       "      <td>0.037455</td>\n",
       "      <td>-0.577069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>-0.168756</td>\n",
       "      <td>-0.369210</td>\n",
       "      <td>-0.048255</td>\n",
       "      <td>-1.346250</td>\n",
       "      <td>-1.352658</td>\n",
       "      <td>-0.777935</td>\n",
       "      <td>-0.196532</td>\n",
       "      <td>-0.474693</td>\n",
       "      <td>-0.808870</td>\n",
       "      <td>-0.089675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689052</td>\n",
       "      <td>-0.993908</td>\n",
       "      <td>-0.696926</td>\n",
       "      <td>-0.920326</td>\n",
       "      <td>-0.583857</td>\n",
       "      <td>-0.793404</td>\n",
       "      <td>-0.573950</td>\n",
       "      <td>-0.529149</td>\n",
       "      <td>-0.637018</td>\n",
       "      <td>0.202861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         period  significance         n    median     wmean      wstd  \\\n",
       "0      1.795541     -0.411116  1.056842 -0.080996 -0.070772 -0.729371   \n",
       "1     -0.169144     -0.404608 -0.609176  1.717669  1.745705  0.605940   \n",
       "2     -0.168778     -0.339781 -0.709640  1.355583  1.383824  0.625651   \n",
       "3     -0.167949     -0.153682 -0.575688  0.376213  0.371072  0.035347   \n",
       "4     -0.167962     -0.326416 -0.475225 -0.960383 -0.960730 -0.642459   \n",
       "...         ...           ...       ...       ...       ...       ...   \n",
       "69995 -0.168765      0.561914  0.043836  0.225089  0.251542  0.162925   \n",
       "69996 -0.168783      0.347649 -0.575688 -0.698025 -0.672249  0.112984   \n",
       "69997 -0.169044     -0.325871 -0.684524  0.065781  0.067812 -0.388981   \n",
       "69998 -0.169061     -0.310516 -0.684524  0.418150  0.422739 -0.051247   \n",
       "69999 -0.168756     -0.369210 -0.048255 -1.346250 -1.352658 -0.777935   \n",
       "\n",
       "        chi2red      roms  norm_peak_to_peak_amp  norm_excess_var  ...  \\\n",
       "0     -0.204919 -0.618402              -0.556206        -0.090013  ...   \n",
       "1     -0.203427 -0.567115              -0.058580        -0.067296  ...   \n",
       "2     -0.194344 -0.467194               0.354400        -0.022121  ...   \n",
       "3     -0.164693 -0.161056              -0.089659        -0.064653  ...   \n",
       "4     -0.187080 -0.368344              -0.677252        -0.087730  ...   \n",
       "...         ...       ...                    ...              ...  ...   \n",
       "69995 -0.099879  0.132890               0.162960        -0.048426  ...   \n",
       "69996  0.255438  1.149209              -0.028490        -0.048638  ...   \n",
       "69997 -0.175824 -0.255117              -0.473654        -0.082420  ...   \n",
       "69998 -0.177880 -0.267395              -0.220118        -0.070384  ...   \n",
       "69999 -0.196532 -0.474693              -0.808870        -0.089675  ...   \n",
       "\n",
       "       PS1_DR1__gMeanPSFMagErr  PS1_DR1__rMeanPSFMag  PS1_DR1__rMeanPSFMagErr  \\\n",
       "0                    -0.505132             -0.171156                -0.711477   \n",
       "1                    -0.771984              1.529579                 2.328723   \n",
       "2                     6.291251              1.346526                 0.143841   \n",
       "3                     0.079213              0.424836                 0.335221   \n",
       "4                    -0.414266             -0.674166                -0.685891   \n",
       "...                        ...                   ...                      ...   \n",
       "69995                 0.408199              0.312270                -0.127323   \n",
       "69996                 0.943568             -0.446250                 0.416117   \n",
       "69997                -0.115449              0.177809                 0.047016   \n",
       "69998                 0.203350              0.476170                 0.174543   \n",
       "69999                -0.689052             -0.993908                -0.696926   \n",
       "\n",
       "       PS1_DR1__iMeanPSFMag  PS1_DR1__iMeanPSFMagErr  PS1_DR1__zMeanPSFMag  \\\n",
       "0                 -0.108664                -0.679382             -0.035408   \n",
       "1                  1.432994                 0.796992              1.331472   \n",
       "2                  1.067136                 0.790237              0.829973   \n",
       "3                  0.589397                -0.315000              0.697595   \n",
       "4                 -0.570005                -0.178704             -0.493979   \n",
       "...                     ...                      ...                   ...   \n",
       "69995              0.312456                -0.503739              0.356175   \n",
       "69996             -0.382705                 0.143071             -0.370791   \n",
       "69997              0.365860                -0.339711              0.478386   \n",
       "69998              0.662414                 0.414358              0.802405   \n",
       "69999             -0.920326                -0.583857             -0.793404   \n",
       "\n",
       "       PS1_DR1__zMeanPSFMagErr  PS1_DR1__yMeanPSFMag  PS1_DR1__yMeanPSFMagErr  \\\n",
       "0                    -0.589965              0.087552                -0.428986   \n",
       "1                     0.413797              1.111349                -0.121882   \n",
       "2                     0.215383              0.617988                -0.170997   \n",
       "3                    -0.091769              0.706519                 0.258703   \n",
       "4                    -0.272985             -0.272220                -0.641119   \n",
       "...                        ...                   ...                      ...   \n",
       "69995                 0.212744              0.361095                -0.558559   \n",
       "69996                -0.387639             -0.145787                 0.548454   \n",
       "69997                -0.262794              0.483536                -0.033011   \n",
       "69998                -0.355381              0.747981                 0.037455   \n",
       "69999                -0.573950             -0.529149                -0.637018   \n",
       "\n",
       "       PS1_DR1__qualityFlag  \n",
       "0                  0.202861  \n",
       "1                 -0.577069  \n",
       "2                  0.202861  \n",
       "3                 -0.577069  \n",
       "4                  0.202861  \n",
       "...                     ...  \n",
       "69995              0.202861  \n",
       "69996              0.202861  \n",
       "69997              0.202861  \n",
       "69998             -0.577069  \n",
       "69999              0.202861  \n",
       "\n",
       "[70000 rows x 69 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the labels\n",
    "df_labels = df_labels.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Special case: Convert string to float\n",
    "df_feats['AllWISE__ph_qual'] = LabelEncoder().fit_transform(df_feats['AllWISE__ph_qual'])\n",
    "\n",
    "# Normalization\n",
    "df_feats_values = df_feats.values\n",
    "df_feats_values_scaled = StandardScaler().fit_transform(df_feats_values)\n",
    "df_feats_scaled = pd.DataFrame(df_feats_values_scaled, columns=df_feats.columns)\n",
    "df_feats_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "rs=8581\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_feats_scaled.values, df_labels.values, test_size=0.5, \n",
    "                                                    random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNN model consists of three parts:\n",
    "1. Autoencoder, which has a encoder that compresses the information and a decoder that reconstructs the information. \n",
    "2. Classifiers: for each label, we have one independent classifier.\n",
    "3. CNN, which is commonly-used in image processing area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a simple classifier for single label\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, label_name_):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=16, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=16)\n",
    "        self.outputfc = nn.Linear(in_features=16, out_features=df_labels[label_name_].nunique())\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.outputfc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_0(nn.Module):\n",
    "    def __init__(self, model_list:list):\n",
    "        super().__init__()\n",
    "        # ==============AUTOENCODER================\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=69, out_features=64)\n",
    "        self.enc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.enc_out = nn.Linear(in_features=32, out_features=16) # output encoded tensor\n",
    "        # decoder\n",
    "        self.dec1 = nn.Linear(in_features=16, out_features=32)\n",
    "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.dec_out = nn.Linear(in_features=64, out_features=69) # output decoded data\n",
    "        # ==========================================\n",
    "        \n",
    "        self.classifiermodels=[]\n",
    "        for i in range(len(model_list)):\n",
    "            self.classifiermodels.append(model_list[i])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        enc_data = F.relu(self.enc1(x))\n",
    "        enc_data = F.relu(self.enc2(enc_data))\n",
    "        enc_data = self.enc_out(enc_data)\n",
    "        \n",
    "        dec_data = F.relu(self.dec1(enc_data))\n",
    "        dec_data = F.relu(self.dec2(dec_data))\n",
    "        dec_data = self.dec_out(dec_data)\n",
    "        \n",
    "        outlist=[]\n",
    "        for i in range(len(model_list)):\n",
    "            outlist.append(self.classifiermodels[i].forward(enc_data))\n",
    "        \n",
    "        return dec_data, outlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LABELS_COL_TO_TRAIN = ['variable']\n",
    "model_list=[classifier(label_name_) for label_name_ in LABELS_COL_TO_TRAIN]\n",
    "model = MLP_0(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86591"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in model, from https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/25\n",
    "sum(p.numel() for p in model.parameters())+sum([sum(p.numel() for p in model_.parameters()) for model_ in model_list])+sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# YOUR CODE HERE\n",
    "epochs = ...\n",
    "learning_rate = ...\n",
    "\n",
    "# DataLoader \n",
    "train_data = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).long())  # Pytorch uses FP32\n",
    "train_ldr = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=False)\n",
    "test_data = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).long())  # Pytorch uses FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifierparaml=[]\n",
    "for model_element in model_list:\n",
    "    classifierparaml+=(model_element.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion_MSE = nn.MSELoss()\n",
    "criterion_CE = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())\n",
    "                             +list(classifierparaml), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with the sum of two loss functions (MSE and CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "for model_s in model.classifiermodels:\n",
    "    model_s.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    for (batch_idx, batch) in enumerate(train_ldr):\n",
    "        X_train_batch = batch[0]\n",
    "        y_train_batch = batch[1]\n",
    "\n",
    "        X_reco, outlist_ = model.forward(X_train_batch)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        loss_1 = criterion_MSE(...\n",
    "        loss_2 = 0.\n",
    "        for j in range(len(LABELS_COL_TO_TRAIN)):\n",
    "            # YOUR CODE HERE\n",
    "            loss_2 += criterion_CE(...\n",
    "            #print(loss)\n",
    "        loss = loss_1 + loss_2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            #print(\"Epoch: {}, batch: {} Loss: {} label_loss:{}\".format(i, batch_idx, loss, label_loss_))\n",
    "            print(\"Epoch: {}, batch: {} Loss1: {:0.4f} Loss2: {:0.4f} Loss: {:0.4f}\".format(i, batch_idx, loss_1, loss_2 ,loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    X_reco_final, outlist_final = model.forward(torch.tensor(X_test).float())\n",
    "print(\"The prediction result: \\n\", X_reco_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluation:\n",
    "# Calculate the accuracy for each class\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    acc_list=[]\n",
    "\n",
    "    for i in range(y_true.shape[1]):\n",
    "        print(y_true[:, i], y_pred[:, i])\n",
    "        acc_list.append(accuracy_score(y_true[:, i], y_pred[:, i]))\n",
    "\n",
    "    return np.array(acc_list)\n",
    "\n",
    "scorelist=[(accuracy_score(F.softmax(outlist_final[iii], dim=1).argmax(dim=1).cpu(), y_test[:, iii])) for iii in range(len(LABELS_COL_TO_TRAIN))]\n",
    "for iii in range(len(LABELS_COL_TO_TRAIN)):\n",
    "    print(LABELS_COL_TO_TRAIN[iii], scorelist[iii])\n",
    "\n",
    "# Evaluation\n",
    "print(\"The mean accuracy: \", np.mean(scorelist))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(scorelist, bins=np.linspace(0., 1, 50))\n",
    "plt.title(\"distribution of acccuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Hierarchial model: modify the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the reconstruction and classification loss functions, we may add a \"dependence loss\", which is hierarchy-related and is regarded as a punishment when predictions are not consistent with the category structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_loss(y_pred1, y_pred2, y_pred3):\n",
    "    loss_ = torch.bitwise_and(y_pred1[:,L1_label.index(\"eclipsing\")]==0, y_pred2[:,L2_label.index(\"EA\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"eclipsing\")]==0, y_pred2[:,L2_label.index(\"EB\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"eclipsing\")]==0, y_pred2[:,L2_label.index(\"EW\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"pulsator\")]==0, y_pred2[:,L2_label.index(\"Delta Scu\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"pulsator\")]==0, y_pred2[:,L2_label.index(\"Cepheid\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"pulsator\")]==0, y_pred2[:,L2_label.index(\"RR Lyrae\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"pulsator\")]==0, y_pred2[:,L2_label.index(\"LPV\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred2[:,L2_label.index(\"LPV\")]==0, y_pred3[:,L3_label.index(\"Mira\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred2[:,L2_label.index(\"LPV\")]==0, y_pred3[:,L3_label.index(\"SRV\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"binary star\")]==0, y_pred2[:,L2_label.index(\"W Uma\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"binary star\")]==0, y_pred2[:,L2_label.index(\"Beta Lyr\")]!=0).sum()+\\\n",
    "    torch.bitwise_and(y_pred1[:,L1_label.index(\"binary star\")]==0, y_pred2[:,L2_label.index(\"RS CVn\")]!=0).sum()\n",
    "    # ...\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total loss function then becomes $$L_{reco}+\\alpha\\times L_{class}+ \\beta \\times L_{dep}$$\n",
    "where $\\alpha$ and $\\beta$ are weights for different loss functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
